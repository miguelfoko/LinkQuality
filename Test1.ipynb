{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "stating link:  0 ==> 18\n"
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-4d9b504ddd8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[1;31m#print(\"Accuracy of RSSI [lr,lsvm,rf,svm] = \",my_general_predictor(df,0.25,\"rssi\",path))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m \u001b[0mmy_general_predictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m \u001b[1;31m#my_general_random_forest_with_threads(df,0.25,'pdr',path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;31m#executor(df,2, 47, 0.25,my_execution_list1,path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-4d9b504ddd8a>\u001b[0m in \u001b[0;36mmy_general_predictor\u001b[1;34m(df, size_of_test, kind, path)\u001b[0m\n\u001b[0;32m    538\u001b[0m                      \u001b[1;31m#   features, labels=rus.fit_resample(features, labels)#The ressampling strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m                      \u001b[1;31m#   labels=pd.Series(labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m                     \u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msize_of_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    541\u001b[0m                     \u001b[0mtaill\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mtaill\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Programmes\\miniconca3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2120\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2121\u001b[0m     n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n\u001b[1;32m-> 2122\u001b[1;33m                                               default_test_size=0.25)\n\u001b[0m\u001b[0;32m   2123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Programmes\\miniconca3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   1803\u001b[0m             \u001b[1;34m'resulting train set will be empty. Adjust any of the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1804\u001b[0m             'aforementioned parameters.'.format(n_samples, test_size,\n\u001b[1;32m-> 1805\u001b[1;33m                                                 train_size)\n\u001b[0m\u001b[0;32m   1806\u001b[0m         )\n\u001b[0;32m   1807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=1, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_rows=10\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import xgboost\n",
    "import graphviz\n",
    "import matplotlib.dates as md\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import scipy\n",
    "from scipy.stats import spearmanr\n",
    "from pylab import rcParams\n",
    "import seaborn as sb\n",
    "from matplotlib.pylab import rcParams\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "from statsmodels.tsa.stattools import adfuller, arma_order_select_ic\n",
    "from sklearn.metrics import r2_score, mean_squared_error, confusion_matrix\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "#import MyAnalysis \n",
    "rcParams['figure.figsize'] = 3,3\n",
    "\n",
    "#Import for threading\n",
    "\n",
    "import random\n",
    "import sys\n",
    "from threading import Thread, RLock\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "df = pd.read_table(\"grenoble_71.k7\",sep = ',',header = 0)\n",
    "\n",
    "def trace_conf_mat(cm, acc,classes, norm, title,cmap=plt.cm.Blues):\n",
    "    accuracy=acc\n",
    "    if norm:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title+'%.2f' % accuracy+' \\n precision=0.63\\n f1-score=0.36',\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    #plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "     #        rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if norm else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "def plot_confusion_matrix_pdr(df, classes, cm,accuracy, normalize=False, title=None,cmap=plt.cm.Blues):\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "    trace_conf_mat(cm, accuracy,classes, normalize, title,cmap=plt.cm.Blues)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "def my_split(df,src,dst,size_of_test,kind):\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    rus = RandomUnderSampler(random_state=0)\n",
    "    target=list()\n",
    "    target_names=list()\n",
    "    \n",
    "    if kind==\"all\":                                              #it is for pdr+rssi\n",
    "        combined_features=list()\n",
    "        for i in range(len(df)):\n",
    "            combined_features.append(df['pdr'][i]*df['pdr'][i]+df['mean_rssi'][i]*df['mean_rssi'][i])\n",
    "            if df['pdr'][i] <= 0.3:\n",
    "                target.append(0)\n",
    "                target_names.append('Bad')\n",
    "            elif df['pdr'][i]>=0.75:\n",
    "                target.append(2)\n",
    "                target_names.append('Good')\n",
    "            elif ((df['pdr'][i]>0.3 and df['pdr'][i]<0.75)and df['mean_rssi'][i]<=-87):\n",
    "                target.append(0)\n",
    "                target_names.append('Bad')\n",
    "            else:\n",
    "                target.append(1)\n",
    "                target_names.append('Interm.')\n",
    "\n",
    "        df['combined_features']=combined_features\n",
    "        colonne=['combined_features']\n",
    "    elif kind==\"pdr\":                                            #It is for pdr\n",
    "        for i in range(len(df)):\n",
    "            if df['pdr'][i] <= 0.3 :\n",
    "                target.append(0)\n",
    "                target_names.append('Bad')\n",
    "            elif df['pdr'][i]>=0.75 :\n",
    "                target.append(2)\n",
    "                target_names.append('Good')\n",
    "            else:\n",
    "                target.append(1)\n",
    "                target_names.append('Interm.')\n",
    "            colonne=['pdr']\n",
    "    else:                                                          #It is for mean_rssi\n",
    "        for i in range(len(df)):\n",
    "            if df['mean_rssi'][i] <= -87 :\n",
    "                target.append(0)\n",
    "                target_names.append('Bad')\n",
    "            elif df['mean_rssi'][i]>=-85 :\n",
    "                target.append(2)\n",
    "                target_names.append('Good')\n",
    "            else:\n",
    "                target.append(1)\n",
    "                target_names.append('Interm.')\n",
    "        colonne=['mean_rssi']\n",
    "    df['target']=target\n",
    "    df['target_names']=target_names\n",
    "    colonne2=['target_names']\n",
    "    df=df.loc[(df['src']==src)&(df['dst']==dst)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#Logistic regression Classification\n",
    "def my_logreg(df1,src,dst,size_of_test,kind,path):#Kind can be \"pdr\", \"mean_rssi\" or \"all\"\n",
    "    df=my_split(df1,src,dst,size_of_test,kind)\n",
    "    text=\"\"\n",
    "    subtitle=\"Accuracy = \"\n",
    "    if kind==\"all\":\n",
    "        title=path+\"LR\\\\Log_Reg_CM_ALL_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        title2=path+\"LR\\\\Log_Reg_CM_Not_Normalized_ALL_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        colonne=['combined_features']\n",
    "    elif kind==\"pdr\":\n",
    "        title=path+\"LR\\\\Log_Reg_CM_PDR_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        title2=path+\"LR\\\\Log_Reg_CM_Not_Normalized_PDR_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        colonne=['pdr']\n",
    "    else:\n",
    "        title=path+\"LR\\\\Log_Reg_CM_RSSI_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        title2=path+\"LR\\\\Log_Reg_CM_Not_Normalized_RSSI_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        colonne=['mean_rssi']\n",
    "\n",
    "    general_test_labels=list()\n",
    "    general_pred_labels=list()\n",
    "    channel_list=df1['channel'].unique()\n",
    "    classes=df['target_names'].unique()\n",
    "    accuracy_results=list()\n",
    "    for i in range (len(channel_list)):\n",
    "        channel_i=channel_list[i]\n",
    "        colonne2=['target_names']\n",
    "        ts=df.loc[(df['channel']==channel_i),colonne]\n",
    "        ts2=df.loc[(df['channel']==channel_i),colonne2]\n",
    "\n",
    "        features=ts.values\n",
    "        labels=ts2['target_names']\n",
    "        classes=labels.unique()\n",
    "        #if len(classes)>1:\n",
    "         #   features, labels=rus.fit_resample(features, labels)#The ressampling strategy\n",
    "         #   labels=pd.Series(labels)\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = size_of_test, random_state=None,shuffle=False)\n",
    "        taill=len(pd.Series(train_labels).unique())\n",
    "        if taill>1:\n",
    "            LogReg=LogisticRegression()\n",
    "            LogReg.fit(train_features,train_labels)\n",
    "            pred_labels=LogReg.predict(test_features)\n",
    "            general_test_labels=general_test_labels+list(test_labels)\n",
    "            general_pred_labels=general_pred_labels+list(pred_labels)\n",
    "        else:\n",
    "            general_test_labels=general_test_labels+list(test_labels)\n",
    "            general_pred_labels=general_pred_labels+list(test_labels)\n",
    "    cm=confusion_matrix(general_test_labels,general_pred_labels)\n",
    "    som=0\n",
    "    taill=len(cm)\n",
    "    total=0\n",
    "    for i in range(taill):\n",
    "        som=som+cm[i][i]\n",
    "        for j in range(taill):\n",
    "            total=total+cm[i][j]\n",
    "        \n",
    "    accuracy=som/total\n",
    "    plot_confusion_matrix_pdr(df, classes,cm,accuracy, normalize=True,title=subtitle)\n",
    "    \n",
    "    plt.savefig(title, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.clf()\n",
    "    \n",
    "    plot_confusion_matrix_pdr(df, classes,cm,accuracy, normalize=False,title=subtitle)\n",
    "    plt.savefig(title2, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.clf()\n",
    "    return accuracy\n",
    "    \n",
    "    \n",
    "#SVM Classification\n",
    "def my_svm(df1,src,dst,size_of_test,kind,path):#Kind can be \"pdr\", \"mean_rssi\" or \"all\"\n",
    "    df=my_split(df1,src,dst,size_of_test,kind)\n",
    "    subtitle=\"Accuracy = \"\n",
    "    if kind==\"all\":\n",
    "            title=path+\"SVM\\\\SVM_CM_ALL_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "            title2=path+\"SVM\\\\SVM_CM_Not_Normalized_ALL_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "            colonne=['combined_features']\n",
    "    elif kind==\"pdr\":\n",
    "        title=path+\"SVM\\\\SVM_CM_PDR_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        title2=path+\"SVM\\\\SVM_CM_Not_Normalized_PDR_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        colonne=['pdr']\n",
    "    else:\n",
    "        title=path+\"SVM\\\\SVM_CM_RSSI_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        title2=path+\"SVM\\\\SVM_CM_Not_Normalized_RSSI_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        colonne=['mean_rssi']\n",
    "\n",
    "    general_test_labels=list()\n",
    "    general_pred_labels=list()\n",
    "    channel_list=df1['channel'].unique()\n",
    "    classes=df['target_names'].unique()\n",
    "    accuracy_results=list()\n",
    "    for i in range (len(channel_list)):\n",
    "        channel_i=channel_list[i]\n",
    "        colonne2=['target_names']\n",
    "        ts=df.loc[(df['channel']==channel_i),colonne]\n",
    "        ts2=df.loc[(df['channel']==channel_i),colonne2]\n",
    "\n",
    "        features=ts.values\n",
    "        labels=ts2['target_names']\n",
    "        #classes=labels.unique()\n",
    "        #if len(classes)>1:\n",
    "        #    features, labels=rus.fit_resample(features, labels)#The ressampling strategy\n",
    "        #    labels=pd.Series(labels)\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = size_of_test, random_state=None,shuffle=True)\n",
    "        taill=len(pd.Series(train_labels).unique())\n",
    "        if taill>1:\n",
    "            clf = SVC(random_state=0, tol=1e-5)\n",
    "            clf.fit(train_features, train_labels)\n",
    "            pred_labels=clf.predict(test_features)\n",
    "            general_test_labels=general_test_labels+list(test_labels)\n",
    "            general_pred_labels=general_pred_labels+list(pred_labels)\n",
    "        else:\n",
    "            general_test_labels=general_test_labels+list(test_labels)\n",
    "            general_pred_labels=general_pred_labels+list(test_labels)\n",
    "    cm=confusion_matrix(general_test_labels,general_pred_labels)\n",
    "    som=0\n",
    "    taill=len(cm)\n",
    "    total=0\n",
    "    for i in range(taill):\n",
    "        som=som+cm[i][i]\n",
    "        for j in range(taill):\n",
    "            total=total+cm[i][j]\n",
    "        \n",
    "    accuracy=som/total\n",
    "    plot_confusion_matrix_pdr(df, classes,cm,accuracy, normalize=True,title=subtitle)\n",
    "    \n",
    "    plt.savefig(title, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.clf()\n",
    "    \n",
    "    plot_confusion_matrix_pdr(df, classes,cm,accuracy, normalize=False,title=subtitle)\n",
    "    plt.savefig(title2, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.clf()\n",
    "    return accuracy\n",
    "    \n",
    "#Linear SVM Classification\n",
    "def my_linear_svm(df1,src,dst,size_of_test,kind,path):#Kind can be \"pdr\", \"mean_rssi\" or \"all\"\n",
    "    df=my_split(df1,src,dst,size_of_test,kind)\n",
    "    subtitle=\"Accuracy = \"\n",
    "    if kind==\"all\":\n",
    "        title=path+\"LSVM\\\\LSVM_CM_ALL_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        title2=path+\"LSVM\\\\LSVM_CM_Not_Normalized_ALL_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        colonne=['combined_features']\n",
    "    elif kind==\"pdr\":\n",
    "        title=path+\"LSVM\\\\LSVM_CM_PDR_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        title2=path+\"LSVM\\\\LSVM_CM_Not_Normalized_PDR_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        colonne=['pdr']\n",
    "    else:\n",
    "        title=path+\"LSVM\\\\LSVM_CM_RSSI_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        title2=path+\"LSVM\\\\LSVM_CM_Not_Normalized_RSSI_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        colonne=['mean_rssi']\n",
    "\n",
    "    general_test_labels=list()\n",
    "    general_pred_labels=list()\n",
    "    channel_list=df1['channel'].unique()\n",
    "    classes=df['target_names'].unique()\n",
    "    accuracy_results=list()\n",
    "    for i in range (len(channel_list)):\n",
    "        channel_i=channel_list[i]\n",
    "        colonne2=['target_names']\n",
    "        ts=df.loc[(df['channel']==channel_i),colonne]\n",
    "        ts2=df.loc[(df['channel']==channel_i),colonne2]\n",
    "\n",
    "        features=ts.values\n",
    "        labels=ts2['target_names']\n",
    "        #classes=labels.unique()\n",
    "        #if len(classes)>1:\n",
    "        #    features, labels=rus.fit_resample(features, labels)#The ressampling strategy\n",
    "         #   labels=pd.Series(labels)\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = size_of_test, random_state=None,shuffle=True)\n",
    "        taill=len(pd.Series(train_labels).unique())\n",
    "        if taill>1:\n",
    "            clf = LinearSVC(random_state=0, tol=1e-5)\n",
    "            clf.fit(train_features, train_labels)\n",
    "            pred_labels=clf.predict(test_features)\n",
    "            general_test_labels=general_test_labels+list(test_labels)\n",
    "            general_pred_labels=general_pred_labels+list(pred_labels)\n",
    "        else:\n",
    "            general_test_labels=general_test_labels+list(test_labels)\n",
    "            general_pred_labels=general_pred_labels+list(test_labels)\n",
    "    cm=confusion_matrix(general_test_labels,general_pred_labels)\n",
    "    som=0\n",
    "    taill=len(cm)\n",
    "    total=0\n",
    "    for i in range(taill):\n",
    "        som=som+cm[i][i]\n",
    "        for j in range(taill):\n",
    "            total=total+cm[i][j]\n",
    "        \n",
    "    accuracy=som/total\n",
    "    plot_confusion_matrix_pdr(df, classes,cm,accuracy, normalize=True,title=subtitle)\n",
    "    \n",
    "    plt.savefig(title, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.clf()\n",
    "    \n",
    "    plot_confusion_matrix_pdr(df, classes,cm,accuracy, normalize=False,title=subtitle)\n",
    "    plt.savefig(title2, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.clf()\n",
    "    return accuracy\n",
    "    \n",
    "#Random Forest Classification\n",
    "def my_random_forest(df1,src,dst,size_of_test,kind,path):#Kind can be \"pdr\", \"mean_rssi\" or \"all\"\n",
    "    df=my_split(df1,src,dst,size_of_test,kind)\n",
    "    subtitle=\"Accuracy = \"\n",
    "    if kind==\"all\":\n",
    "        title=path+\"RF\\\\RF_CM_ALL_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        title2=path+\"RF\\\\RF_CM_Not_Normalized_ALL_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        colonne=['combined_features']\n",
    "    elif kind==\"pdr\":\n",
    "        title=path+\"RF\\\\RF_CM_PDR_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        title2=path+\"RF\\\\RF_CM_Not_Normalized_PDR_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        colonne=['pdr']\n",
    "    else:\n",
    "        title=path+\"RF\\\\RF_CM_RSSI_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        title2=path+\"RF\\\\RF_CM_Not_Normalized_RSSI_\"+str(src)+\"===\"+str(dst)+\".png\"\n",
    "        colonne=['mean_rssi']\n",
    "\n",
    "    general_test_labels=list()\n",
    "    general_pred_labels=list()\n",
    "    channel_list=df1['channel'].unique()\n",
    "    classes=df['target_names'].unique()\n",
    "    accuracy_results=list()\n",
    "    for i in range (len(channel_list)):\n",
    "        channel_i=channel_list[i]\n",
    "        colonne2=['target_names']\n",
    "        ts=df.loc[(df['channel']==channel_i),colonne]\n",
    "        ts2=df.loc[(df['channel']==channel_i),colonne2]\n",
    "\n",
    "        features=ts.values\n",
    "        labels=ts2['target_names']\n",
    "        #classes=labels.unique()\n",
    "        #if len(classes)>1:\n",
    "        #    features, labels=rus.fit_resample(features, labels)#The ressampling strategy\n",
    "        #    labels=pd.Series(labels)\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = size_of_test, random_state=None,shuffle=True)\n",
    "        taill=len(pd.Series(train_labels).unique())\n",
    "        if taill>1:\n",
    "            rf = RandomForestClassifier(n_estimators=1000, max_depth=5, random_state=0)\n",
    "            rf.fit(train_features, train_labels)\n",
    "            pred_labels=rf.predict(test_features)\n",
    "            general_test_labels=general_test_labels+list(test_labels)\n",
    "            general_pred_labels=general_pred_labels+list(pred_labels)\n",
    "        else:\n",
    "            general_test_labels=general_test_labels+list(test_labels)\n",
    "            general_pred_labels=general_pred_labels+list(test_labels)\n",
    "    cm=confusion_matrix(general_test_labels,general_pred_labels)\n",
    "    som=0\n",
    "    taill=len(cm)\n",
    "    total=0\n",
    "    for i in range(taill):\n",
    "        som=som+cm[i][i]\n",
    "        for j in range(taill):\n",
    "            total=total+cm[i][j]\n",
    "        \n",
    "    accuracy=som/total\n",
    "    plot_confusion_matrix_pdr(df, classes,cm,accuracy, normalize=True,title=subtitle)\n",
    "    \n",
    "    plt.savefig(title, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.clf()\n",
    "    \n",
    "    plot_confusion_matrix_pdr(df, classes,cm,accuracy, normalize=False,title=subtitle)\n",
    "    plt.savefig(title2, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.clf()\n",
    "    return accuracy\n",
    "    \n",
    "\n",
    "def executor(df,src, dst, size_of_test,model_list,path):\n",
    "    import os\n",
    "    file_name=path+\"final_results.csv\"\n",
    "    exists=os.path.isfile(file_name)\n",
    "    if exists:\n",
    "        file=open(file_name,\"a+\")\n",
    "        #file.write(str('\\n')+'model,link,test_size,accuracy_with_pdr_and_rssi,accuracy_with_pdr,accuracy_with_rssi')\n",
    "    else:\n",
    "        file=open(file_name,\"a\")\n",
    "        file.write(str('\\n')+'model,link,test_size,accuracy_with_pdr_and_rssi,accuracy_with_pdr,accuracy_with_rssi')\n",
    "    for i in model_list:\n",
    "        if i=='lr':\n",
    "            lg_all=my_logreg(df,src,dst,size_of_test,'all',path)\n",
    "            lg_pdr=my_logreg(df,src,dst,size_of_test,'pdr',path)\n",
    "            lg_rssi=my_logreg(df,src,dst,size_of_test,'rssi',path)\n",
    "            file.write(str('\\n')+'LogReg,'+str(src)+'==='+str(dst)+','+str(size_of_test)+','+str(lg_all)+','+str(lg_pdr)+','+str(lg_rssi))\n",
    "        elif i=='lsvm':\n",
    "            lsvm_all=my_linear_svm(df,src,dst,size_of_test,'all',path)\n",
    "            lsvm_pdr=my_linear_svm(df,src,dst,size_of_test,'pdr',path)\n",
    "            lsvm_rssi=my_linear_svm(df,src,dst,size_of_test,'rssi',path)\n",
    "            file.write(str('\\n')+'LSVM,'+str(src)+'==='+str(dst)+','+str(size_of_test)+','+str(lsvm_all)+','+str(lsvm_pdr)+','+str(lsvm_rssi))\n",
    "        elif i=='svm':\n",
    "            svm_all=my_svm(df,src,dst,size_of_test,'all',path)\n",
    "            svm_pdr=my_svm(df,src,dst,size_of_test,'pdr',path)\n",
    "            svm_rssi=my_svm(df,src,dst,size_of_test,'rssi',path)\n",
    "            file.write(str('\\n')+'SVM,'+str(src)+'==='+str(dst)+','+str(size_of_test)+','+str(svm_all)+','+str(svm_pdr)+','+str(svm_rssi))\n",
    "        elif i=='rf':  \n",
    "            rf_all=my_random_forest(df,src,dst,size_of_test,'all',path)\n",
    "            rf_pdr=my_random_forest(df,src,dst,size_of_test,'pdr',path)\n",
    "            rf_rssi=my_random_forest(df,src,dst,size_of_test,'rssi',path)\n",
    "            file.write(str('\\n')+'RF,'+str(src)+'==='+str(dst)+','+str(size_of_test)+','+str(rf_all)+','+str(rf_pdr)+','+str(rf_rssi))\n",
    "    file.close()\n",
    "        \n",
    "        \n",
    "#The following function aims to compute the accuracies of all the links in the network for all our methods\n",
    "def final_executor(df,size_of_test,my_execution_list,path):\n",
    "    senders=df['src'].unique()\n",
    "    receivers=df['dst'].unique()\n",
    "    \n",
    "    for sender in senders:\n",
    "        for receiver in receivers:\n",
    "            colonne=[\"pdr\"]\n",
    "            ts=df.loc[(df['src']==sender)&(df['dst']==receiver),colonne]\n",
    "            if len(ts)>0:\n",
    "                print(\"Starting link \",sender,\"===\",receiver)\n",
    "                executor(df,sender,receiver,size_of_test,my_execution_list,path)\n",
    "                print(\"Ending link \",sender,\"===\",receiver)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def my_general_predictor(df,size_of_test,kind,path):#Kind can be \"pdr\", \"rssi\" or \"all\"\n",
    "    subtitle=\"Accuracy = \"\n",
    "    if kind==\"all\":\n",
    "        lr_title=path+\"\\\\LR_CM_ALL.png\"\n",
    "        lr_title2=path+\"\\\\LR_CM_ALL_Not_Normalized.png\"\n",
    "        \n",
    "        rf_title=path+\"\\\\RF_CM_ALL.png\"\n",
    "        rf_title2=path+\"\\\\RF_CM_ALL_Not_Normalized.png\"\n",
    "        \n",
    "        svm_title=path+\"\\\\SVM_CM_ALL.png\"\n",
    "        svm_title2=path+\"\\\\SVM_ALL_Not_Normalized.png\"\n",
    "        \n",
    "        lsvm_title=path+\"\\\\LSVM_CM_ALL.png\"\n",
    "        lsvm_title2=path+\"\\\\LSVM_CM_ALL_Not_Normalized.png\"\n",
    "        \n",
    "        colonne=['combined_features']\n",
    "        \n",
    "    elif kind==\"pdr\":\n",
    "        lr_title=path+\"\\\\GENERAL\\\\LR_CM_PDR.png\"\n",
    "        lr_title2=path+\"\\\\LR_CM_PDR_Not_Normalized.png\"\n",
    "        \n",
    "        rf_title=path+\"\\\\RF_CM_PDR.png\"\n",
    "        rf_title2=path+\"\\\\RF_CM_PDR_Not_Normalized.png\"\n",
    "        \n",
    "        svm_title=path+\"\\\\SVM_CM_PDR.png\"\n",
    "        svm_title2=path+\"\\\\SVM_CM_PDR_Not_Normalized.png\"\n",
    "        \n",
    "        lsvm_title=path+\"\\\\LSVM_CM_PDR.png\"\n",
    "        lsvm_title2=path+\"\\\\LSVM_CM_PDR_Not_Normalized.png\"\n",
    "        colonne=['pdr']\n",
    "        \n",
    "    else:\n",
    "        lr_title=path+\"\\\\LR_CM_RSSI.png\"\n",
    "        lr_title2=path+\"\\\\LR_CM_RSSI_Not_Normalized.png\"\n",
    "        \n",
    "        rf_title=path+\"\\\\RF_CM_RSSI.png\"\n",
    "        rf_title2=path+\"\\\\RF_CM_RSSI_Not_Normalized.png\"\n",
    "        \n",
    "        svm_title=path+\"\\\\SVM_CM_RSSI.png\"\n",
    "        svm_title2=path+\"\\\\SVM_CM_RSSI_Not_Normalized.png\"\n",
    "        \n",
    "        lsvm_title=path+\"\\\\LSVM_CM_RSSI.png\"\n",
    "        lsvm_title2=path+\"\\\\LSVM_CM_RSSI_Not_Normalized.png\"\n",
    "        colonne=['mean_rssi']\n",
    "\n",
    "        \n",
    "    lr_general_test_labels=list()\n",
    "    lr_general_pred_labels=list()  \n",
    "    \n",
    "    rf_general_test_labels=list()\n",
    "    rf_general_pred_labels=list()\n",
    "    \n",
    "    svm_general_test_labels=list()\n",
    "    svm_general_pred_labels=list()\n",
    "    \n",
    "    lsvm_general_test_labels=list()\n",
    "    lsvm_general_pred_labels=list()\n",
    "    \n",
    "    senders=df['src'].unique()\n",
    "    receivers=df['dst'].unique()\n",
    "    \n",
    "    for sender in senders:\n",
    "        for receiver in receivers:\n",
    "            ts=my_split(df,sender,receiver,size_of_test,kind)\n",
    "            if len(ts)>0:\n",
    "                print('stating link: ',sender,'==>',receiver)\n",
    "                channel_list=ts['channel'].unique()\n",
    "                classes=ts['target_names'].unique()\n",
    "                for i in range (len(channel_list)):\n",
    "                    channel_i=channel_list[i]\n",
    "                    colonne2=['target_names']\n",
    "                    ts1=ts.loc[(ts['channel']==channel_i),colonne]\n",
    "                    ts2=ts.loc[(ts['channel']==channel_i),colonne2]\n",
    "\n",
    "                    features=ts1.values\n",
    "                    labels=ts2['target_names']\n",
    "                    #classes=labels.unique()\n",
    "                    #if len(classes)>1:\n",
    "                     #   features, labels=rus.fit_resample(features, labels)#The ressampling strategy\n",
    "                     #   labels=pd.Series(labels)\n",
    "                    train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = size_of_test, random_state=None,shuffle=False)\n",
    "                    taill=len(pd.Series(train_labels).unique())\n",
    "                    if taill>1:\n",
    "                        LogReg=LogisticRegression()\n",
    "                        LogReg.fit(train_features,train_labels)\n",
    "                        lr_pred_labels=LogReg.predict(test_features)\n",
    "                        lr_general_test_labels=lr_general_test_labels+list(test_labels)\n",
    "                        lr_general_pred_labels=lr_general_pred_labels+list(lr_pred_labels)\n",
    "                        \n",
    "                        lsvm = LinearSVC(random_state=0, tol=1e-5)\n",
    "                        lsvm.fit(train_features, train_labels)\n",
    "                        lsvm_pred_labels = lsvm.predict(test_features)\n",
    "                        lsvm_general_test_labels=lsvm_general_test_labels+list(test_labels)\n",
    "                        lsvm_general_pred_labels=lsvm_general_pred_labels+list(lsvm_pred_labels)\n",
    "                        \n",
    "                        svm = SVC(random_state=0, tol=1e-5)\n",
    "                        svm.fit(train_features, train_labels)\n",
    "                        svm_pred_labels = svm.predict(test_features)\n",
    "                        svm_general_test_labels=svm_general_test_labels+list(test_labels)\n",
    "                        svm_general_pred_labels=svm_general_pred_labels+list(svm_pred_labels)\n",
    "                        \n",
    "                        rf = RandomForestClassifier(n_estimators=1000, max_depth=5, random_state=0)\n",
    "                        rf.fit(train_features, train_labels)\n",
    "                        rf_pred_labels = rf.predict(test_features)\n",
    "                        rf_general_test_labels=rf_general_test_labels+list(test_labels)\n",
    "                        rf_general_pred_labels=rf_general_pred_labels+list(rf_pred_labels)\n",
    "                        \n",
    "                    else:\n",
    "                        lr_general_test_labels=lr_general_test_labels+list(test_labels)\n",
    "                        lr_general_pred_labels=lr_general_pred_labels+list(test_labels)\n",
    "                        \n",
    "                        rf_general_test_labels=rf_general_test_labels+list(test_labels)\n",
    "                        rf_general_pred_labels=rf_general_pred_labels+list(test_labels)\n",
    "                        \n",
    "                        svm_general_test_labels=svm_general_test_labels+list(test_labels)\n",
    "                        svm_general_pred_labels=svm_general_pred_labels+list(test_labels)\n",
    "                        \n",
    "                        lsvm_general_test_labels=lsvm_general_test_labels+list(test_labels)\n",
    "                        lsvm_general_pred_labels=lsvm_general_pred_labels+list(test_labels)\n",
    "                print('ending link: ',sender,'==>',receiver)\n",
    "    lr_cm=confusion_matrix(lr_general_test_labels,lr_general_pred_labels)\n",
    "    \n",
    "    svm_cm=confusion_matrix(svm_general_test_labels,svm_general_pred_labels)\n",
    "    \n",
    "    lsvm_cm=confusion_matrix(lsvm_general_test_labels,lsvm_general_pred_labels)\n",
    "    \n",
    "    rf_cm=confusion_matrix(rf_general_test_labels,rf_general_pred_labels)\n",
    "    \n",
    "    lr_som=0\n",
    "    lsvm_som=0\n",
    "    rf_som=0\n",
    "    svm_som=0\n",
    "    taill=len(lr_cm)\n",
    "    total=0\n",
    "    for i in range(taill):\n",
    "        lr_som=lr_som+lr_cm[i][i]\n",
    "        svm_som=svm_som+svm_cm[i][i]\n",
    "        lsvm_som=lsvm_som+lsvm_cm[i][i]\n",
    "        rf_som=rf_som+rf_cm[i][i]\n",
    "        \n",
    "        for j in range(taill):\n",
    "            total=total+lr_cm[i][j]\n",
    "        \n",
    "    lr_accuracy=lr_som/total\n",
    "    lsvm_accuracy=lsvm_som/total\n",
    "    svm_accuracy=svm_som/total\n",
    "    rf_accuracy=rf_som/total\n",
    "    \n",
    "    plot_confusion_matrix_pdr(df, classes,lr_cm,lr_accuracy, normalize=True,title=subtitle)\n",
    "    plot_confusion_matrix_pdr(df, classes,rf_cm,rf_accuracy, normalize=True,title=subtitle)\n",
    "    plot_confusion_matrix_pdr(df, classes,lsvm_cm,lsvm_accuracy, normalize=True,title=subtitle)\n",
    "    plot_confusion_matrix_pdr(df, classes,svm_cm,svm_accuracy, normalize=True,title=subtitle)\n",
    "    \n",
    "    plt.savefig(lr_title, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.savefig(lsvm_title, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.savefig(svm_title, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.savefig(rf_title, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.clf()\n",
    "    \n",
    "    plot_confusion_matrix_pdr(df, classes,lr_cm,lr_accuracy, normalize=False,title=subtitle)\n",
    "    plot_confusion_matrix_pdr(df, classes,lsvm_cm,lsvm_accuracy, normalize=False,title=subtitle)\n",
    "    plot_confusion_matrix_pdr(df, classes,svm_cm,svm_accuracy, normalize=False,title=subtitle)\n",
    "    plot_confusion_matrix_pdr(df, classes,rf_cm,rf_accuracy, normalize=False,title=subtitle)\n",
    "    \n",
    "    plt.savefig(lr_title2, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.savefig(lsvm_title2, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.savefig(svm_title2, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.savefig(rf_title2, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    plt.clf()\n",
    "    return lr_accuracy,lsvm_accuracy,rf_accuracy,svm_accuracy\n",
    "\n",
    "myLoc = RLock() #A lock to synchronize writing operations inside test and pred labels\n",
    "class MyThread(Thread):\n",
    "    def __init__(self,ts,general_test_labels,general_pred_labels,colonne,size_of_test,model):\n",
    "        Thread.__init__(self)\n",
    "        self.ts=ts\n",
    "        self.general_test_labels=general_test_labels\n",
    "        self.general_pred_labels=general_pred_labels\n",
    "        self.colonne=colonne\n",
    "        self.size_of_test=size_of_test\n",
    "        self.model=model\n",
    "        \n",
    "    def run(self):\n",
    "        if len(self.ts)>0:\n",
    "            channel_list=self.ts['channel'].unique()\n",
    "            classes=self.ts['target_names'].unique()\n",
    "            for i in range (len(channel_list)):\n",
    "                channel_i=channel_list[i]\n",
    "                colonne2=['target_names']\n",
    "                ts1=self.ts.loc[(self.ts['channel']==channel_i),self.colonne]\n",
    "                ts2=self.ts.loc[(self.ts['channel']==channel_i),colonne2]\n",
    "\n",
    "                features=ts1.values\n",
    "                labels=ts2['target_names']\n",
    "                #classes=labels.unique()\n",
    "                #if len(classes)>1:\n",
    "                 #   features, labels=rus.fit_resample(features, labels)#The ressampling strategy\n",
    "                 #   labels=pd.Series(labels)\n",
    "                train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = self.size_of_test, random_state=None,shuffle=True)\n",
    "                taill=len(pd.Series(train_labels).unique())\n",
    "                if taill>1:\n",
    "                    if self.model=='lsvm':\n",
    "                        lsvm = LinearSVC(random_state=0, tol=1e-5)\n",
    "                        lsvm.fit(train_features, train_labels)\n",
    "                        pred_labels=lsvm.predict(test_features)\n",
    "                    elif self.model=='rf':\n",
    "                        rf = RandomForestClassifier(n_estimators=1000, max_depth=5, random_state=0)\n",
    "                        rf.fit(train_features, train_labels)\n",
    "                        pred_labels=rf.predict(test_features)\n",
    "                    elif self.model=='svm':\n",
    "                        clf = SVC(random_state=0, tol=1e-5)\n",
    "                        clf.fit(train_features, train_labels)\n",
    "                        pred_labels=clf.predict(test_features)\n",
    "                    else:\n",
    "                        LogReg=LogisticRegression()\n",
    "                        LogReg.fit(train_features,train_labels)\n",
    "                        pred_labels=LogReg.predict(test_features)\n",
    "                    with myLoc:\n",
    "                        self.general_test_labels=self.general_test_labels+list(test_labels)\n",
    "                        self.general_pred_labels=self.general_pred_labels+list(pred_labels)\n",
    "                else:\n",
    "                    with myLoc:\n",
    "                        self.general_test_labels=self.general_test_labels+list(test_labels)\n",
    "                        self.general_pred_labels=self.general_pred_labels+list(test_labels)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "def my_particular_split(df,src,dst,size_of_test,kind):\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    rus = RandomUnderSampler(random_state=0)\n",
    "    target=list()\n",
    "    target_names=list()\n",
    "    \n",
    "    if kind==\"all\":                                              #it is for pdr+rssi\n",
    "        combined_features=list()\n",
    "        for i in range(len(df)):\n",
    "            combined_features.append(df['pdr'][i]*df['pdr'][i]+df['mean_rssi'][i]*df['mean_rssi'][i])\n",
    "            if df['pdr'][i] <= 0.3:\n",
    "                target.append(0)\n",
    "                target_names.append('Bad')\n",
    "            elif df['pdr'][i]>=0.75:\n",
    "                target.append(2)\n",
    "                target_names.append('Good')\n",
    "            elif ((df['pdr'][i]>0.3 and df['pdr'][i]<0.75)and df['mean_rssi'][i]<=-87):\n",
    "                target.append(0)\n",
    "                target_names.append('Bad')\n",
    "            else:\n",
    "                target.append(1)\n",
    "                target_names.append('Interm.')\n",
    "\n",
    "        df['combined_features']=combined_features\n",
    "        colonne=['combined_features']\n",
    "    elif kind==\"pdr\":                                            #It is for pdr\n",
    "        for i in range(len(df)):\n",
    "            if df['pdr'][i] <= 0.3 :\n",
    "                target.append(0)\n",
    "                target_names.append('Bad')\n",
    "            elif df['pdr'][i]>=0.75 :\n",
    "                target.append(2)\n",
    "                target_names.append('Good')\n",
    "            else:\n",
    "                target.append(1)\n",
    "                target_names.append('Interm.')\n",
    "            colonne=['pdr']\n",
    "    else:                                                          #It is for mean_rssi\n",
    "        for i in range(len(df)):\n",
    "            if df['mean_rssi'][i] <= -87 :\n",
    "                target.append(0)\n",
    "                target_names.append('Bad')\n",
    "            elif df['mean_rssi'][i]>=-85 :\n",
    "                target.append(2)\n",
    "                target_names.append('Good')\n",
    "            else:\n",
    "                target.append(1)\n",
    "                target_names.append('Interm.')\n",
    "        colonne=['mean_rssi']\n",
    "    df['target']=target\n",
    "    df['target_names']=target_names\n",
    "    colonne2=['target_names']\n",
    "    df=df.loc[(df['src']==src)&(df['dst']==dst)]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def my_general_linear_svm_with_threads(df,size_of_test,kind,path):#Kind can be \"pdr\", \"rssi\" or \"all\"\n",
    "    text=\"\"\n",
    "    subtitle=\"Accuracy = \"\n",
    "    if kind==\"all\":\n",
    "        title=path+\"\\\\LSVM_CM_ALL.png\"\n",
    "        title2=path+\"\\\\LSVM_CM_ALL_Not_Normalized.png\"\n",
    "        colonne=['combined_features']\n",
    "        \n",
    "    elif kind==\"pdr\":\n",
    "        title=path+\"\\\\LSVM_CM_PDR.png\"\n",
    "        title2=path+\"\\\\LSVM_CM_PDR_Not_Normalized.png\"\n",
    "        colonne=['pdr']\n",
    "        \n",
    "    else:\n",
    "        title=path+\"\\\\LSVM_CM_RSSI.png\"\n",
    "        title2=path+\"\\\\LSVM_CM_RSSI_Not_Normalized.png\"\n",
    "        colonne=['mean_rssi']\n",
    "\n",
    "        \n",
    "    general_test_labels=list()\n",
    "    general_pred_labels=list()  \n",
    "    senders=df['src'].unique()\n",
    "    receivers=df['dst'].unique()\n",
    "    \n",
    "    for sender in senders:\n",
    "        for receiver in receivers:\n",
    "            print('starting link: ',sender,'==>',receiver)\n",
    "            ts=my_particular_split(df,sender,receiver,size_of_test,kind)\n",
    "            mySolver=MyThread(ts,general_test_labels,general_pred_labels,colonne,size_of_test,'lsvm')# For each link, we create a new thread to proccess it\n",
    "            mySolver.start()\n",
    "            mySolver.join()\n",
    "            print('ending link: ',sender,'==>',receiver)\n",
    "            \n",
    "    cm=confusion_matrix(general_test_labels,general_pred_labels)\n",
    "    som=0\n",
    "    taill=len(cm)\n",
    "    total=len(general_test_labels)\n",
    "    for i in range(taill):\n",
    "        som=som+cm[i][i]\n",
    "\n",
    "            \n",
    "        \n",
    "    accuracy=som/total\n",
    "    plot_confusion_matrix_pdr(df, classes,cm,accuracy, normalize=True,title=subtitle)\n",
    "    \n",
    "    plt.savefig(title, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.clf()\n",
    "    \n",
    "    plot_confusion_matrix_pdr(df, classes,cm,accuracy, normalize=False,title=subtitle)\n",
    "    plt.savefig(title2, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.clf()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def my_general_random_forest_with_threads(df,size_of_test,kind,path):#Kind can be \"pdr\", \"rssi\" or \"all\"\n",
    "    text=\"\"\n",
    "    subtitle=\"Accuracy = \"\n",
    "    if kind==\"all\":\n",
    "        title=\"C:\\\\Users\\WSN-LINK\\\\Documents\\\\TEST\\\\SOCALE\\\\CHOOSE\\\\FINAL\\\\GENERAL\\\\RF_CM_ALL.png\"\n",
    "        title2=\"C:\\\\Users\\WSN-LINK\\\\Documents\\\\TEST\\\\SOCALE\\\\CHOOSE\\\\FINAL\\\\GENERAL\\\\RF_CM_ALL_Not_Normalized.png\"\n",
    "        colonne=['combined_features']\n",
    "        \n",
    "    elif kind==\"pdr\":\n",
    "        title=\"C:\\\\Users\\WSN-LINK\\\\Documents\\\\TEST\\\\SOCALE\\\\CHOOSE\\\\FINAL\\\\GENERAL\\\\RF_CM_PDR.png\"\n",
    "        title2=\"C:\\\\Users\\WSN-LINK\\\\Documents\\\\TEST\\\\SOCALE\\\\CHOOSE\\\\FINAL\\\\GENERAL\\\\RF_CM_PDR_Not_Normalized.png\"\n",
    "        colonne=['pdr']\n",
    "        \n",
    "    else:\n",
    "        title=\"C:\\\\Users\\WSN-LINK\\\\Documents\\\\TEST\\\\SOCALE\\\\CHOOSE\\\\FINAL\\\\GENERAL\\\\RF_CM_RSSI.png\"\n",
    "        title2=\"C:\\\\Users\\WSN-LINK\\\\Documents\\\\TEST\\\\SOCALE\\\\CHOOSE\\\\FINAL\\\\GENERAL\\\\RF_CM_RSSI_Not_Normalized.png\"\n",
    "        colonne=['mean_rssi']\n",
    "\n",
    "        \n",
    "    general_test_labels=list()\n",
    "    general_pred_labels=list()  \n",
    "    senders=df['src'].unique()\n",
    "    receivers=df['dst'].unique()\n",
    "    \n",
    "    for sender in senders:\n",
    "        for receiver in receivers:\n",
    "            print('starting link: ',sender,'==>',receiver)\n",
    "            ts=my_particular_split(df,sender,receiver,size_of_test,kind)\n",
    "            mySolver=MyThread(ts,general_test_labels,general_pred_labels,colonne,size_of_test,'rf')# For each link, we create a new thread to proccess it\n",
    "            mySolver.start()\n",
    "            mySolver.join()\n",
    "            print('ending link: ',sender,'==>',receiver)\n",
    "            \n",
    "    cm=confusion_matrix(general_test_labels,general_pred_labels)\n",
    "    som=0\n",
    "    taill=len(cm)\n",
    "    total=len(general_test_labels)\n",
    "    for i in range(taill):\n",
    "        som=som+cm[i][i]\n",
    "\n",
    "            \n",
    "        \n",
    "    accuracy=som/total\n",
    "    plot_confusion_matrix_pdr(df, classes,cm,accuracy, normalize=True,title=subtitle)\n",
    "    \n",
    "    plt.savefig(title, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.clf()\n",
    "    \n",
    "    plot_confusion_matrix_pdr(df, classes,cm,accuracy, normalize=False,title=subtitle)\n",
    "    plt.savefig(title2, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.clf()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ros = RandomOverSampler(random_state=0)\n",
    "#rus = RandomUnderSampler(random_state=0)\n",
    "path=\"C:\\\\Users\\Hp\\\\Documents\\\\TEST\\\\FINAL\\\\GENERAL\\\\\"\n",
    "path2=\"C:\\\\Users\\WSN-LINK\\\\Documents\\\\TEST\\\\SOCALE\\\\CHOOSE\\\\FINAL\\\\\"\n",
    "my_execution_list=['lr', 'lsvm','svm','rf']\n",
    "my_execution_list1=['lr']\n",
    "\n",
    "#print(\"Accuracy of PDR+RSSI [lr,lsvm,rf,svm] = \",my_general_predictor(df,0.25,\"all\",path))\n",
    "#print(\"Accuracy of PDR [lr,lsvm,rf,svm] = \",my_general_predictor(df,0.25,\"pdr\",path))\n",
    "#print(\"Accuracy of RSSI [lr,lsvm,rf,svm] = \",my_general_predictor(df,0.25,\"rssi\",path))\n",
    "\n",
    "my_general_predictor(df,0.25,all,path)\n",
    "#my_general_random_forest_with_threads(df,0.25,'pdr',path)\n",
    "#executor(df,2, 47, 0.25,my_execution_list1,path)\n",
    "#executor(df,0, 20, 0.25,my_execution_list1,path)\n",
    "#final_executor(df,0.25,my_execution_list,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}